{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [
        "UW_1CQh6Sa68",
        "3xMftD0CS49y"
      ],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMY2yhvHACZf68tY1tZxwpF",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/krishnasaiv/search_tool/blob/main/Part_2_Semantic_Search_with_ML_and_BERT/2_1_Semantic_Search_Engine_with_Faiss_and_DistilBERT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Install Necessary Libraries"
      ],
      "metadata": {
        "id": "UW_1CQh6Sa68"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cLTUpLqcM7pr"
      },
      "outputs": [],
      "source": [
        "! pip install transformers torch\n",
        "! pip install faiss-gpu\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Download Distilled BERT model "
      ],
      "metadata": {
        "id": "3xMftD0CS49y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModel, AutoTokenizer\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
        "model = AutoModel.from_pretrained(\"distilbert-base-uncased\")"
      ],
      "metadata": {
        "id": "6A663daxSVjy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Read Data"
      ],
      "metadata": {
        "id": "6v8g-yHsTCml"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "\n",
        "with open('/content/sentences.json') as file:\n",
        "    sentences = json.load(file)\n",
        "\n",
        "with open('/content/questions.json') as file:\n",
        "    questions = json.load(file)\n"
      ],
      "metadata": {
        "id": "6mBogESZTFPd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "questions[:3]"
      ],
      "metadata": {
        "id": "J9yoPOb6TaEK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentences[:3]"
      ],
      "metadata": {
        "id": "TIgwwqFAUGnP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Encode Documents into Vectors"
      ],
      "metadata": {
        "id": "MsGTt2YEUdSq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoModel, AutoTokenizer\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
        "model = AutoModel.from_pretrained(\"distilbert-base-uncased\")\n",
        "\n",
        "\n",
        "def encode(document: str) -> torch.Tensor:\n",
        "  tokens = tokenizer(document, return_tensors='pt')\n",
        "  vector = model(**tokens)[0].detach().squeeze()\n",
        "  return torch.mean(vector, dim=0)\n",
        "\n",
        "\n",
        "averaged_vectors = [encode(sentence) for sentence in sentences]\n",
        "\n",
        "[v.size() for v in averaged_vectors]"
      ],
      "metadata": {
        "id": "brEO5nqLUci8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. Upload to In Memory Faiss Vector Databse"
      ],
      "metadata": {
        "id": "bn4IqLJaV7tF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import faiss\n",
        "import numpy as np\n",
        "\n",
        "index = faiss.IndexIDMap(faiss.IndexFlatIP(768)) \n",
        "\n",
        "index.add_with_ids(\n",
        "    np.array([t.numpy() for t in averaged_vectors]),\n",
        "    # the IDs will be 0 to len(documents)\n",
        "    np.array(range(0, len(sentences)))\n",
        ")\n"
      ],
      "metadata": {
        "id": "HUOSbG2NUybL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6. Search Functionality\n",
        "\n",
        "\n",
        "*   Vectorize Query\n",
        "*   Rank Documents\n",
        "*   Retriece Results\n",
        "\n"
      ],
      "metadata": {
        "id": "MAWzLwzqWCUm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def search(query: str, k=1):\n",
        "  encoded_query = encode(query).unsqueeze(dim=0).numpy()\n",
        "  top_k = index.search(encoded_query, k)\n",
        "  scores = top_k[0][0]\n",
        "  results = [sentences[_id] for _id in top_k[1][0]]\n",
        "  return list(zip(results, scores))\n",
        "\n",
        "questions[0], search(questions[0])"
      ],
      "metadata": {
        "id": "3akjfXNJVtIC"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}